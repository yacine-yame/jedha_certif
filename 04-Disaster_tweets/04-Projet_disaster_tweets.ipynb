{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = ['text','target'], keep = 'first', inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>881</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>898</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>1723</td>\n",
       "      <td>buildings%20burning</td>\n",
       "      <td>Mackay, QLD, Australia</td>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1752</td>\n",
       "      <td>buildings%20burning</td>\n",
       "      <td>Epic City, BB.</td>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>1760</td>\n",
       "      <td>buildings%20burning</td>\n",
       "      <td>dallas</td>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>1922</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>Mackay, QLD, Australia</td>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>1950</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>dallas</td>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1968</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>Epic City, BB.</td>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>4068</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>4076</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>4656</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>4659</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>5662</td>\n",
       "      <td>floods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>5699</td>\n",
       "      <td>floods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>5996</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>6012</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>6017</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>6031</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>6087</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Prophet (peace be upon him) said 'Save you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>6088</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hellfire is surrounded by desires so be carefu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>6094</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>Jubail IC, Saudi Arabia.</td>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>6097</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Prophet (peace be upon him) said 'Save you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>6112</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hellfire is surrounded by desires so be carefu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>6113</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hellfire! We donÛªt even want to think about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>6123</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>?????? ???? ??????</td>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>6134</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>Riyadh ')</td>\n",
       "      <td>Hellfire! We donÛªt even want to think about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>6220</td>\n",
       "      <td>hijacker</td>\n",
       "      <td>worldwide</td>\n",
       "      <td>RT NotExplained: The only known image of infam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>6223</td>\n",
       "      <td>hijacker</td>\n",
       "      <td>worldwide</td>\n",
       "      <td>RT NotExplained: The only known image of infam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535</th>\n",
       "      <td>6537</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>6566</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>8018</td>\n",
       "      <td>refugees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>8044</td>\n",
       "      <td>refugees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>8698</td>\n",
       "      <td>sinking</td>\n",
       "      <td>Every Where in the World</td>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>8702</td>\n",
       "      <td>sinking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6533</th>\n",
       "      <td>9470</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>Jeddah_Saudi Arabia.</td>\n",
       "      <td>In #islam saving a person is equal in reward t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6535</th>\n",
       "      <td>9472</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>Riyadh</td>\n",
       "      <td>In #islam saving a person is equal in reward t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id              keyword                  location  \\\n",
       "606    881         bioterrorism                       NaN   \n",
       "620    898         bioterrorism                       NaN   \n",
       "1190  1723  buildings%20burning    Mackay, QLD, Australia   \n",
       "1207  1752  buildings%20burning            Epic City, BB.   \n",
       "1214  1760  buildings%20burning                    dallas   \n",
       "1323  1922  burning%20buildings    Mackay, QLD, Australia   \n",
       "1337  1950  burning%20buildings                    dallas   \n",
       "1351  1968  burning%20buildings            Epic City, BB.   \n",
       "2802  4068            displaced  Pedophile hunting ground   \n",
       "2803  4076            displaced  Pedophile hunting ground   \n",
       "3209  4656             engulfed                       NaN   \n",
       "3212  4659             engulfed                   Kuwait    \n",
       "3936  5662               floods                       NaN   \n",
       "3964  5699               floods                       NaN   \n",
       "4171  5996            hazardous                       NaN   \n",
       "4182  6012            hazardous                       NaN   \n",
       "4185  6017            hazardous                       NaN   \n",
       "4193  6031            hazardous          New Delhi, Delhi   \n",
       "4233  6087             hellfire                       NaN   \n",
       "4234  6088             hellfire                       NaN   \n",
       "4238  6094             hellfire  Jubail IC, Saudi Arabia.   \n",
       "4239  6097             hellfire                       NaN   \n",
       "4249  6112             hellfire                       NaN   \n",
       "4250  6113             hellfire                       NaN   \n",
       "4253  6123             hellfire        ?????? ???? ??????   \n",
       "4259  6134             hellfire                 Riyadh ')   \n",
       "4317  6220             hijacker                 worldwide   \n",
       "4319  6223             hijacker                 worldwide   \n",
       "4535  6537               injury                       NaN   \n",
       "4554  6566               injury                       NaN   \n",
       "5552  8018             refugees                       NaN   \n",
       "5573  8044             refugees                       NaN   \n",
       "6018  8698              sinking  Every Where in the World   \n",
       "6021  8702              sinking                       NaN   \n",
       "6533  9470            terrorism      Jeddah_Saudi Arabia.   \n",
       "6535  9472            terrorism                    Riyadh   \n",
       "\n",
       "                                                   text  target  \n",
       "606                          To fight bioterrorism sir.       1  \n",
       "620                          To fight bioterrorism sir.       0  \n",
       "1190  Mmmmmm I'm burning.... I'm burning buildings I...       1  \n",
       "1207  I Pledge Allegiance To The P.O.P.E. And The Bu...       0  \n",
       "1214  like for the music video I want some real acti...       1  \n",
       "1323  Mmmmmm I'm burning.... I'm burning buildings I...       0  \n",
       "1337  like for the music video I want some real acti...       0  \n",
       "1351  I Pledge Allegiance To The P.O.P.E. And The Bu...       1  \n",
       "2802  .POTUS #StrategicPatience is a strategy for #G...       1  \n",
       "2803  .POTUS #StrategicPatience is a strategy for #G...       0  \n",
       "3209  He came to a land which was engulfed in tribal...       0  \n",
       "3212  He came to a land which was engulfed in tribal...       1  \n",
       "3936  Who is bringing the tornadoes and floods. Who ...       1  \n",
       "3964  Who is bringing the tornadoes and floods. Who ...       0  \n",
       "4171  #foodscare #offers2go #NestleIndia slips into ...       1  \n",
       "4182  Caution: breathing may be hazardous to your he...       1  \n",
       "4185  Caution: breathing may be hazardous to your he...       0  \n",
       "4193  #foodscare #offers2go #NestleIndia slips into ...       0  \n",
       "4233  The Prophet (peace be upon him) said 'Save you...       0  \n",
       "4234  Hellfire is surrounded by desires so be carefu...       0  \n",
       "4238  #Allah describes piling up #wealth thinking it...       0  \n",
       "4239  The Prophet (peace be upon him) said 'Save you...       1  \n",
       "4249  Hellfire is surrounded by desires so be carefu...       1  \n",
       "4250  Hellfire! We donÛªt even want to think about ...       0  \n",
       "4253  #Allah describes piling up #wealth thinking it...       1  \n",
       "4259  Hellfire! We donÛªt even want to think about ...       1  \n",
       "4317  RT NotExplained: The only known image of infam...       0  \n",
       "4319  RT NotExplained: The only known image of infam...       1  \n",
       "4535  CLEARED:incident with injury:I-495  inner loop...       1  \n",
       "4554  CLEARED:incident with injury:I-495  inner loop...       0  \n",
       "5552  wowo--=== 12000 Nigerian refugees repatriated ...       1  \n",
       "5573  wowo--=== 12000 Nigerian refugees repatriated ...       0  \n",
       "6018  that horrible sinking feeling when youÛªve be...       1  \n",
       "6021  that horrible sinking feeling when youÛªve be...       0  \n",
       "6533  In #islam saving a person is equal in reward t...       0  \n",
       "6535  In #islam saving a person is equal in reward t...       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_record = df[df.duplicated(['text'], keep=False)]\n",
    "duplicates_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = ['text'], keep = False, inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, keyword, location, text, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_record = df[df.duplicated(['text'], keep=False)]\n",
    "duplicates_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       56\n",
       "location    2472\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7485, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>10863</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7481</th>\n",
       "      <td>10864</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7482</th>\n",
       "      <td>10866</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7483</th>\n",
       "      <td>10869</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7484</th>\n",
       "      <td>10873</td>\n",
       "      <td>missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7485 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword location  \\\n",
       "0         1  missing      NaN   \n",
       "1         4  missing      NaN   \n",
       "2         5  missing      NaN   \n",
       "3         6  missing      NaN   \n",
       "4         7  missing      NaN   \n",
       "...     ...      ...      ...   \n",
       "7480  10863  missing      NaN   \n",
       "7481  10864  missing      NaN   \n",
       "7482  10866  missing      NaN   \n",
       "7483  10869  missing      NaN   \n",
       "7484  10873  missing      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7480  #WorldNews Fallen powerlines on G:link tram: U...       1  \n",
       "7481  on the flip side I'm at Walmart and there is a...       1  \n",
       "7482  Suicide bomber kills 15 in Saudi security site...       1  \n",
       "7483  Two giant cranes holding a bridge collapse int...       1  \n",
       "7484  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7485 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'] = np.where(df['keyword'].isna(),'missing', df['keyword'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4297\n",
       "1    3188\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['keyword','text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>missing</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword                                               text  target\n",
       "0  missing  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1  missing             Forest fire near La Ronge Sask. Canada       1\n",
       "2  missing  All residents asked to 'shelter in place' are ...       1\n",
       "3  missing  13,000 people receive #wildfires evacuation or...       1\n",
       "4  missing  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 17:24:11.291755: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-01-01 17:24:11.291805: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spacy and french initialisation\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Stop words \n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-4b0479e6ce56>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"text_clean\"] = df[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
      "<ipython-input-16-4b0479e6ce56>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
      "<ipython-input-16-4b0479e6ce56>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)]))\n"
     ]
    }
   ],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)]))\n",
    "\n",
    "df[\"keyword_clean\"] = df[\"keyword\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "df[\"keyword_clean\"] = df[\"keyword_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "df[\"keyword_clean\"] = df[\"keyword_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"text_clean\"].isna()==False\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean']=df['text_clean'].apply(remove_URL)\n",
    "df['text_clean']=df['text_clean'].apply(remove_emoji)\n",
    "# df['text_clean']=df['text_clean'].apply(correct_spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text_clean']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7485/7485 [00:00<00:00, 8123.95it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=tf.reshape(df.target,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000) # instanciate the tokenizer\n",
    "tokenizer.fit_on_texts(df[\"text_clean\"])\n",
    "df[\"text_encoded\"] = tokenizer.texts_to_sequences(df.text_clean)\n",
    "df[\"len_review\"] = df[\"text_encoded\"].apply(lambda x: len(x))\n",
    "\n",
    "df[\"keyword_encoded\"] = tokenizer.texts_to_sequences(df.keyword_clean)\n",
    "df[\"len_keyword\"] = df[\"keyword_encoded\"].apply(lambda x: len(x))\n",
    "\n",
    "df = df[df[\"len_review\"]!=0]\n",
    "df = df[df[\"len_keyword\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pad = tf.keras.preprocessing.sequence.pad_sequences(df.text_encoded, padding=\"post\", maxlen=10)\n",
    "keyword_pad = tf.keras.preprocessing.sequence.pad_sequences(df.keyword_encoded, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6250, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = tf.data.Dataset.from_tensor_slices((text_pad, df.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((10,), ()), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "TAKE_SIZE = int(0.8*df.shape[0])\n",
    "\n",
    "train_data = full_ds.take(TAKE_SIZE).shuffle(TAKE_SIZE)\n",
    "train_data = train_data.batch(16)\n",
    "\n",
    "test_data = full_ds.skip(TAKE_SIZE)\n",
    "test_data = test_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  2 349 603   0   0   0   0   0   0   0]\n",
      " [  4   3   0   0   0   0   0   0   0   0]\n",
      " [ 95   0   0   0   0   0   0   0   0   0]\n",
      " [199 340 764 369 446  82 431   0   0   0]\n",
      " [ 31  97 687   0   0   0   0   0   0   0]\n",
      " [982 603 855 303  73  28 396   1  63   0]\n",
      " [458 555 555 235 251   0   0   0   0   0]\n",
      " [ 91 432 244   0   0   0   0   0   0   0]\n",
      " [ 55 189 736 867 188 579 477   0   0   0]\n",
      " [120 388   0   0   0   0   0   0   0   0]\n",
      " [122 506   0   0   0   0   0   0   0   0]\n",
      " [395 660 764 312 150 152   0   0   0   0]\n",
      " [ 32 876 330   2 407   0   0   0   0   0]\n",
      " [ 63  10 780 254 243 468  13   0   0   0]\n",
      " [124   6  41 197  10  87 694   0   0   0]\n",
      " [535 120  93   0   0   0   0   0   0   0]], shape=(16, 10), dtype=int32) tf.Tensor([0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1], shape=(16,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    " # Regardons un batch \n",
    "for text, targ in train_data.take(1):\n",
    "  print(text, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20345"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, GRU, LSTM, SpatialDropout1D, Dropout\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 10\n",
    "\n",
    "model=tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Input(shape=[max_len]))\n",
    "model.add(tf.keras.layers.Embedding(max_words,128,input_length=max_len))    \n",
    "\n",
    "model.add(tf.keras.layers.LSTM(200, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(200,return_sequences=True))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(200))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "          \n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid')) #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 128)           128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 200)           263200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,084,513\n",
      "Trainable params: 1,084,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 17s 32ms/step - loss: 0.5253 - binary_accuracy: 0.7536 - val_loss: 0.4846 - val_binary_accuracy: 0.7656\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4266 - binary_accuracy: 0.8218 - val_loss: 0.5282 - val_binary_accuracy: 0.7592\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.4038 - binary_accuracy: 0.8330 - val_loss: 0.5496 - val_binary_accuracy: 0.7520\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 0.3830 - binary_accuracy: 0.8392 - val_loss: 0.5454 - val_binary_accuracy: 0.7664\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3607 - binary_accuracy: 0.8480 - val_loss: 0.5048 - val_binary_accuracy: 0.7624\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.3355 - binary_accuracy: 0.8564 - val_loss: 0.7358 - val_binary_accuracy: 0.7280\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 9s 30ms/step - loss: 0.3053 - binary_accuracy: 0.8744 - val_loss: 0.8064 - val_binary_accuracy: 0.7344\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.2818 - binary_accuracy: 0.8800 - val_loss: 0.9656 - val_binary_accuracy: 0.7456\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 0.2566 - binary_accuracy: 0.8892 - val_loss: 0.7846 - val_binary_accuracy: 0.7488\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.2329 - binary_accuracy: 0.8992 - val_loss: 1.0995 - val_binary_accuracy: 0.7312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29064552130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(text_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred']=abs(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categ (n):\n",
    "    if n <0.5:\n",
    "        val=0\n",
    "    else:\n",
    "        val=1\n",
    "    return val\n",
    "\n",
    "df['pred2']=df['pred'].apply(categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>len_review</th>\n",
       "      <th>keyword_encoded</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>miss</td>\n",
       "      <td>[408, 176]</td>\n",
       "      <td>2</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>miss</td>\n",
       "      <td>[116, 3, 162, 507, 999]</td>\n",
       "      <td>5</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[8, 67, 201, 299, 36]</td>\n",
       "      <td>5</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[262, 177, 122, 167, 67, 112]</td>\n",
       "      <td>6</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>missing</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update   california hwy 20 close dir...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[168, 36, 441, 315, 822, 773, 275, 3, 67]</td>\n",
       "      <td>9</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7479</th>\n",
       "      <td>missing</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "      <td>official quarantine place alabama home possibl...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[205, 96, 330, 935, 29, 378, 853, 397]</td>\n",
       "      <td>8</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>missing</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>worldnews fall powerline glink tram update fir...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[84, 168, 3, 519, 83, 489, 814]</td>\n",
       "      <td>7</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7482</th>\n",
       "      <td>missing</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicide bomber kill 15 saudi security site mos...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[42, 155, 10, 318, 260, 399, 411, 322, 37, 9, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7483</th>\n",
       "      <td>missing</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>giant crane hold bridge collapse nearby home h...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[590, 920, 308, 249, 38, 481, 29]</td>\n",
       "      <td>7</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7484</th>\n",
       "      <td>missing</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>late home raze northern california wildfire   ...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[99, 29, 352, 134, 36, 67, 525, 9]</td>\n",
       "      <td>8</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5508 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword                                               text  target  \\\n",
       "0     missing  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1     missing             Forest fire near La Ronge Sask. Canada       1   \n",
       "3     missing  13,000 people receive #wildfires evacuation or...       1   \n",
       "4     missing  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "5     missing  #RockyFire Update => California Hwy. 20 closed...       1   \n",
       "...       ...                                                ...     ...   \n",
       "7479  missing  Officials say a quarantine is in place at an A...       1   \n",
       "7480  missing  #WorldNews Fallen powerlines on G:link tram: U...       1   \n",
       "7482  missing  Suicide bomber kills 15 in Saudi security site...       1   \n",
       "7483  missing  Two giant cranes holding a bridge collapse int...       1   \n",
       "7484  missing  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             text_clean keyword_clean  \\\n",
       "0                  deed reason earthquake allah forgive          miss   \n",
       "1                 forest fire near la ronge sask canada          miss   \n",
       "3     13000 people receive wildfire evacuation order...          miss   \n",
       "4     got send photo ruby alaska smoke wildfire pour...          miss   \n",
       "5     rockyfire update   california hwy 20 close dir...          miss   \n",
       "...                                                 ...           ...   \n",
       "7479  official quarantine place alabama home possibl...          miss   \n",
       "7480  worldnews fall powerline glink tram update fir...          miss   \n",
       "7482  suicide bomber kill 15 saudi security site mos...          miss   \n",
       "7483  giant crane hold bridge collapse nearby home h...          miss   \n",
       "7484  late home raze northern california wildfire   ...          miss   \n",
       "\n",
       "                                           text_encoded  len_review  \\\n",
       "0                                            [408, 176]           2   \n",
       "1                               [116, 3, 162, 507, 999]           5   \n",
       "3                                 [8, 67, 201, 299, 36]           5   \n",
       "4                         [262, 177, 122, 167, 67, 112]           6   \n",
       "5             [168, 36, 441, 315, 822, 773, 275, 3, 67]           9   \n",
       "...                                                 ...         ...   \n",
       "7479             [205, 96, 330, 935, 29, 378, 853, 397]           8   \n",
       "7480                    [84, 168, 3, 519, 83, 489, 814]           7   \n",
       "7482  [42, 155, 10, 318, 260, 399, 411, 322, 37, 9, ...          11   \n",
       "7483                  [590, 920, 308, 249, 38, 481, 29]           7   \n",
       "7484                 [99, 29, 352, 134, 36, 67, 525, 9]           8   \n",
       "\n",
       "     keyword_encoded  len_keyword      pred  pred2  \n",
       "0              [121]            1  0.983850      1  \n",
       "1              [121]            1  0.999245      1  \n",
       "3              [121]            1  0.999939      1  \n",
       "4              [121]            1  0.999728      1  \n",
       "5              [121]            1  0.999677      1  \n",
       "...              ...          ...       ...    ...  \n",
       "7479           [121]            1  0.999711      1  \n",
       "7480           [121]            1  0.999949      1  \n",
       "7482           [121]            1  0.989972      1  \n",
       "7483           [121]            1  0.999297      1  \n",
       "7484           [121]            1  0.999970      1  \n",
       "\n",
       "[5508 rows x 11 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['target']==df['pred2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>len_review</th>\n",
       "      <th>keyword_encoded</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>missing</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[440, 330, 287, 201, 330, 299, 375]</td>\n",
       "      <td>7</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>missing</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>m hill fire wood</td>\n",
       "      <td>miss</td>\n",
       "      <td>[4, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.248654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>bbcmtd wholesale market ablaze httptcolhyxeohy6c</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>[264, 442]</td>\n",
       "      <td>2</td>\n",
       "      <td>[442]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.317644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>accident</td>\n",
       "      <td>#TruckCrash Overturns On #FortWorth Interstate...</td>\n",
       "      <td>1</td>\n",
       "      <td>truckcrash overturn fortworth interstate httpt...</td>\n",
       "      <td>accident</td>\n",
       "      <td>[88]</td>\n",
       "      <td>1</td>\n",
       "      <td>[60]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>accident</td>\n",
       "      <td>???? it was an accident http://t.co/Oia5fxi4gM</td>\n",
       "      <td>0</td>\n",
       "      <td>accident httptcooia5fxi4gm</td>\n",
       "      <td>accident</td>\n",
       "      <td>[60]</td>\n",
       "      <td>1</td>\n",
       "      <td>[60]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>missing</td>\n",
       "      <td>#Sismo DETECTADO #JapÌ_n 15:41:07 Seismic inte...</td>\n",
       "      <td>1</td>\n",
       "      <td>sismo detectado japìn 154107 seismic intensity...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[483]</td>\n",
       "      <td>1</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7469</th>\n",
       "      <td>missing</td>\n",
       "      <td>An IS group suicide bomber detonated an explos...</td>\n",
       "      <td>1</td>\n",
       "      <td>group suicide bomber detonate explosivespacked...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[323, 42, 155, 160, 322, 425, 260, 430, 10, 31...</td>\n",
       "      <td>11</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.314901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7475</th>\n",
       "      <td>missing</td>\n",
       "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
       "      <td>1</td>\n",
       "      <td>fatherofthree lose control car overtake collid...</td>\n",
       "      <td>miss</td>\n",
       "      <td>[263, 715, 43, 93]</td>\n",
       "      <td>4</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>missing</td>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>1</td>\n",
       "      <td>siren nt forney tornado warn</td>\n",
       "      <td>miss</td>\n",
       "      <td>[179, 1, 331, 457]</td>\n",
       "      <td>4</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7481</th>\n",
       "      <td>missing</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>flip m walmart bomb evacuate stay tune blow</td>\n",
       "      <td>miss</td>\n",
       "      <td>[4, 17, 83, 363, 74]</td>\n",
       "      <td>5</td>\n",
       "      <td>[121]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>742 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                                               text  target  \\\n",
       "2      missing  All residents asked to 'shelter in place' are ...       1   \n",
       "7      missing  I'm on top of the hill and I can see a fire in...       1   \n",
       "31      ablaze  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n",
       "82    accident  #TruckCrash Overturns On #FortWorth Interstate...       1   \n",
       "89    accident     ???? it was an accident http://t.co/Oia5fxi4gM       0   \n",
       "...        ...                                                ...     ...   \n",
       "7463   missing  #Sismo DETECTADO #JapÌ_n 15:41:07 Seismic inte...       1   \n",
       "7469   missing  An IS group suicide bomber detonated an explos...       1   \n",
       "7475   missing  Father-of-three Lost Control of Car After Over...       1   \n",
       "7478   missing  a siren just went off and it wasn't the Forney...       1   \n",
       "7481   missing  on the flip side I'm at Walmart and there is a...       1   \n",
       "\n",
       "                                             text_clean keyword_clean  \\\n",
       "2     resident ask shelter place notify officer evac...          miss   \n",
       "7                                      m hill fire wood          miss   \n",
       "31     bbcmtd wholesale market ablaze httptcolhyxeohy6c        ablaze   \n",
       "82    truckcrash overturn fortworth interstate httpt...      accident   \n",
       "89                           accident httptcooia5fxi4gm      accident   \n",
       "...                                                 ...           ...   \n",
       "7463  sismo detectado japìn 154107 seismic intensity...          miss   \n",
       "7469  group suicide bomber detonate explosivespacked...          miss   \n",
       "7475  fatherofthree lose control car overtake collid...          miss   \n",
       "7478                       siren nt forney tornado warn          miss   \n",
       "7481        flip m walmart bomb evacuate stay tune blow          miss   \n",
       "\n",
       "                                           text_encoded  len_review  \\\n",
       "2                   [440, 330, 287, 201, 330, 299, 375]           7   \n",
       "7                                                [4, 3]           2   \n",
       "31                                           [264, 442]           2   \n",
       "82                                                 [88]           1   \n",
       "89                                                 [60]           1   \n",
       "...                                                 ...         ...   \n",
       "7463                                              [483]           1   \n",
       "7469  [323, 42, 155, 160, 322, 425, 260, 430, 10, 31...          11   \n",
       "7475                                 [263, 715, 43, 93]           4   \n",
       "7478                                 [179, 1, 331, 457]           4   \n",
       "7481                               [4, 17, 83, 363, 74]           5   \n",
       "\n",
       "     keyword_encoded  len_keyword      pred  pred2  \n",
       "2              [121]            1  0.414820      0  \n",
       "7              [121]            1  0.248654      0  \n",
       "31             [442]            1  0.317644      0  \n",
       "82              [60]            1  0.318912      0  \n",
       "89              [60]            1  0.504342      1  \n",
       "...              ...          ...       ...    ...  \n",
       "7463           [121]            1  0.274744      0  \n",
       "7469           [121]            1  0.314901      0  \n",
       "7475           [121]            1  0.010104      0  \n",
       "7478           [121]            1  0.083323      0  \n",
       "7481           [121]            1  0.462604      0  \n",
       "\n",
       "[742 rows x 11 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['target']!=df['pred2']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
